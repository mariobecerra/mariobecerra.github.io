---
author: "Mario Becerra"
output: pdf_document
---

```{r setup, echo=T, message=F, warning=FALSE, cache = F}
library(MASS)
library(knitr)
library(caret)
library(kableExtra)
library(tidyverse)

opts_chunk$set(echo=T, message=F, warning=FALSE, cache = T)

theme_set(theme_bw())

dat <- twoClassSim(500000,
                   intercept = -40,
                   linearVars = 3,
                   noiseVars = 1)

head(dat)

table(dat$Class)
prop.table(table(dat$Class))
```



## Exploratory Data Analysis

Head of the data.

```{r}
head(dat) %>% 
  kable()
```

Let's see the proportion of cases in response variable:

```{r}
dat %>% 
  ggplot() +
  geom_bar(aes(failure))
```

Seems like we're looking at an imbalanced problem:

```{r}
table(dat$failure)
```

Since we don't have so many variables, we can check for pairwise comparisons in the variables.

```{r}
GGally::ggpairs(sample_n(dat, 20000))
```

We see that attribute 7 and 8 are the same. In fact the sum of squared differences is zero, as can be seen in the next line:

```{r}
sum((dat$attribute7 - dat$attribute8)^2)
```

We can see that attributes 2, 3, 4, 8, and 9 are highly skewed. Let's take a log transformation to work with them.

```{r}
dat_2 <- dat %>% 
  select(-attribute7) %>% 
  mutate(log_attribute2 = log(attribute2 + 1),
         log_attribute3 = log(attribute3 + 1),
         log_attribute4 = log(attribute4 + 1),
         log_attribute8 = log(attribute8 + 1),
         log_attribute9 = log(attribute9 + 1),
         attribute2_cat = ifelse(dat$attribute2 > 0, "(> 0)", "(= 0)"),
         attribute3_cat = ifelse(dat$attribute3 > 0, "(> 0)", "(= 0)"),
         attribute4_cat = ifelse(dat$attribute4 > 0, "(> 0)", "(= 0)"),
         attribute8_cat = ifelse(dat$attribute8 > 0, "(> 0)", "(= 0)"),
         attribute9_cat = ifelse(dat$attribute9 > 0, "(> 0)", "(= 0)"))
```

We can see that the log transformed variables have a lot of zero values.

```{r}
dat_2 %>% 
  select(attribute1, 
         log_attribute2,
         log_attribute3,
         log_attribute4,
         attribute5,
         attribute6,
         log_attribute8,
         log_attribute9) %>% 
  gather(attribute, value) %>% 
  ggplot(aes(x = value)) +
  #geom_histogram(aes(y=..density..)) +
  geom_density(aes(y=..density..)) +
  facet_wrap(~attribute, scales = 'free')
```


Let's see the distribution of the attributes given that there was a failure or not:

```{r}
dat_2 %>% 
  select(failure,
         attribute1, 
         log_attribute2,
         log_attribute3,
         log_attribute4,
         attribute5,
         attribute6,
         log_attribute8,
         log_attribute9) %>% 
  gather(attribute, value, -failure) %>% 
  ggplot() +
  geom_boxplot(aes(factor(failure), value)) +
  facet_wrap(~attribute, scales = "free")
```

And the mean values:

```{r}
dat_2 %>% 
  select(failure,
         attribute1, 
         log_attribute2,
         log_attribute3,
         log_attribute4,
         attribute5,
         attribute6,
         log_attribute8,
         log_attribute9) %>% 
  gather(attribute, value, -failure) %>% 
  group_by(attribute, failure) %>% 
  summarize(mean = mean(value))
```


We can see that for failues, the median value of attribute 1 is slightly higher. For attributes 2, 4 and 8 we can see that higher values are more related with failures.

In the following tables we can see that the proportion of failures is bigger if we condition on each of these attributes being equal or bigger than zero.

```{r}
prop.table(table(dat_2$attribute2_cat, dat_2$failure), margin = 1)
prop.table(table(dat_2$attribute3_cat, dat_2$failure), margin = 1)
prop.table(table(dat_2$attribute4_cat, dat_2$failure), margin = 1)
prop.table(table(dat_2$attribute8_cat, dat_2$failure), margin = 1)
prop.table(table(dat_2$attribute9_cat, dat_2$failure), margin = 1)
```


Now, let's see the weekly and daily number of failures in the period:

```{r}
dat_2 %>% 
  group_by(week = cut(date, "week")) %>% 
  summarize(num_failures = sum(failure == 1)) %>% 
  ggplot() +
  geom_line(aes(week, num_failures, group = 1), size = 0.7, alpha = 0.7) +
  theme(axis.text.x=element_text(angle=90, hjust=1))

dat_2 %>% 
  group_by(date) %>% 
  summarize(num_failures = sum(failure == 1)) %>% 
  ggplot() +
  geom_line(aes(date, num_failures, group = 1), size = 0.7, alpha = 0.7)

```

I just list here some functions that we're gonna use later on.

```{r}

#Colorblind friendly palette
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# Function to compute a ROC curve of a classifier
roc_curve <- function(model, 
                      data, 
                      response_variable, 
                      positive_value,
                      range = c((0:100)/100)){
  resp_var_levels = pull(unique(data[,response_variable]))
  negative_value = setdiff(resp_var_levels, positive_value)
  df <- lapply(range, function(p){
    
    preds <- data %>% 
      select(response_variable) %>% 
      set_names("class") %>% 
      mutate(pred_prob = predict(model, data, type = "prob")[,positive_value]) %>%
      mutate(class_pred = ifelse(pred_prob > p, positive_value, negative_value))
    
    confmat <- confusionMatrix(preds$class_pred,
                               preds$class,
                               positive = positive_value)
    
    measures <- confmat$byClass %>%
      as.data.frame() %>%
      rename_("value" = ".") %>%
      mutate(measure = row.names(.))
    
    sensitivity <- measures %>%
      filter(measure == "Sensitivity") %>%
      .$value
    
    specificity <- measures %>%
      filter(measure == "Specificity") %>%
      .$value
    
    return(
      tibble(
        cutoff = p,
        sens = sensitivity,
        spec = specificity
      )
    )
    
  }) %>% 
    bind_rows()
  
  gg <- ggplot(df,
               aes(1-spec, sens)) +
    geom_line(size = 0.7, alpha = 0.9)+
    labs(title= "ROC curve", 
         x = "False positive rate (1-Specificity)", 
         y = "True positive rate (Sensibility)") +
    theme(legend.position="bottom") +
    #scale_color_manual(values = cbPalette) +
    guides(color=guide_legend(ncol = 1, byrow = TRUE)) +
    geom_abline(slope = 1, color = "grey", size = 0.3) +
    scale_x_continuous(breaks = seq(0, 1, by = 0.05), limits = c(0, 1)) +
    scale_y_continuous(breaks = seq(0, 1, by = 0.05), limits = c(0, 1)) 
  
  return(gg)
}



roc_curve_many <- function(df){
  # Receives a dataframe of the type:
  #  cutoff  sens  spec   model
  #    <dbl> <dbl> <dbl>   <chr>
  # 1   0.00     0     1 log var
  # 2   0.01     0     1 log var
  # 3   0.02     0     1 log var
  gg <- ggplot(df,
               aes(1-spec, sens, color = model)) +
    geom_line(size = 0.7, alpha = 0.9)+
    labs(title= "ROC curve", 
         x = "False positive rate (1-Specificity)", 
         y = "True positive rate (Sensibility)") +
    theme(legend.position="bottom") +
    scale_color_manual(values = cbPalette) +
    guides(color=guide_legend(ncol = 1, byrow = TRUE)) +
    geom_abline(slope = 1, color = "grey", size = 0.3) +
    scale_x_continuous(breaks = seq(0, 1, by = 0.05), limits = c(0, 1)) +
    scale_y_continuous(breaks = seq(0, 1, by = 0.05), limits = c(0, 1)) 
  
  return(gg)
}
```


## Models

Let's divide the dataset in training and test.

```{r}
set.seed(201710)
ix_train <- sample(1:nrow(dat_2), floor(nrow(dat_2)*0.7))

train_dat <- dat_2 %>% 
  slice(ix_train)

test_dat <- dat_2 %>% 
  slice(-ix_train)

trCtrl <- trainControl(method = 'none')
```


#### Binomial regression

We'll first to try simple linear models with a logistic link function, e.g. binomial regression or logistic regression.

Using the log transformed attribute variables. We fit this with the caret package which trains through the \texttt{glmnet} package.

```{r}
# Function to print trained model results
glm_results <- function(model, train_data, test_data, response_variable, positive_value,
                        range = 0:100/100){
  
  train_data <- train_data %>% 
    mutate_("response" = response_variable)
  
  test_data <- test_data %>% 
    mutate_("response" = response_variable)
  
  kappa_accuracy_plot <- model$results %>% 
    mutate(alpha = factor(alpha)) %>% 
    select(lambda, alpha, Accuracy, Kappa) %>% 
    gather(measure, value, Accuracy:Kappa) %>% 
    ggplot(aes(lambda, value, colour = alpha, group = alpha)) +
    geom_point() +
    geom_line() +
    facet_wrap(~measure) +
    ylim(0, 1) 
  
  kappa_plot <- model$results %>% 
    mutate(alpha = factor(alpha)) %>% 
    ggplot(aes(lambda, Kappa, colour = alpha, group = alpha)) +
    geom_point() +
    geom_line()
  
  cm_train <- caret::confusionMatrix(predict(model, newdata = train_data), 
                                     train_data$response,
                                     positive = positive_value)
  
  cm_test <- caret::confusionMatrix(predict(model, newdata = test_data), 
                                    test_data$response,
                                    positive = positive_value)
  
  
  roc_curve_train <- roc_curve(model, train_data, response_variable, positive_value, range)
  
  roc_curve_test <- roc_curve(model, test_data, response_variable, positive_value, range)
  
  return(list(kappa_accuracy_plot, 
              kappa_plot,
              cm_train,
              cm_test,
              roc_curve_train,
              roc_curve_test))
}

```

The following results show a plot of the Cohen's kappa of the model according to the regularization values of the \texttt{glmnet} package. A value of $\alpha = 1$ corresponds to a LASSO penaly and a value of $\alpha = 0$ corresponds to a ridge penalty (in this case, adding regularization isn't benefitial at all). Next, we can see the confusion matrices corresponding to the training and test sets respectively; and finally we can see the ROC curve of the train set and the test set respectively.

```{r}
mod_bin_1 <- train(failure ~ attribute1 +
                     log_attribute2 +
                     log_attribute3 +
                     log_attribute4 +
                     attribute5 +
                     attribute6 +
                     log_attribute8 +
                     log_attribute9,
                   data = train_dat, 
                   method = "glmnet", 
                   tuneGrid = expand.grid(
                     .alpha = 0:1,
                     .lambda = 0:30/10),
                   metric = "Kappa")


results_mod_bin_1 <- glm_results(mod_bin_1, train_dat, test_dat, "failure", "1")

results_mod_bin_1[[2]]
results_mod_bin_1[[3]]
results_mod_bin_1[[4]]
results_mod_bin_1[[5]]
results_mod_bin_1[[6]]


```

We can see that the classifier is not doing a very good job. Almost all cases are being classified as not failure. This is due to the imbalance of the data. Adding weights to the loss function may help to deal with this problem.

```{r}
model_weights_1 <- ifelse(train_dat$failure == "0",
                          (1/table(train_dat$failure)[1]) * 0.5,
                          (1/table(train_dat$failure)[2]) * 0.5)


mod_bin_2_w <- train(failure ~ attribute1 +
                       log_attribute2 +
                       log_attribute3 +
                       log_attribute4 +
                       attribute5 +
                       attribute6 +
                       log_attribute8 +
                       log_attribute9,
                     data = train_dat, 
                     method = "glmnet", 
                     weights = model_weights_1,
                     tuneGrid = expand.grid(
                       .alpha = 0:1,
                       .lambda = seq(0, 0.3, length.out = 5)),
                     metric = "Kappa")

results_mod_bin_2 <- glm_results(mod_bin_2_w, train_dat, test_dat, "failure", "1")

results_mod_bin_2[[2]]
results_mod_bin_2[[3]]
results_mod_bin_2[[4]]
results_mod_bin_2[[5]]
results_mod_bin_2[[6]]


```

The results are better. We sacrified the false positive rate in order to improve the false negative rate. The ROC curve looks better in general, the former was too flat and closer to a random classifier.

Now we turn our attention to the log transformed variables. Instead of treating them as continuous variables, we'll treat them as categorical, whether they're equal or bigger than zero, and then see if this improves the results. We'll keep using the same weights for the loss function optimization.

```{r}

mod_bin_3 <- train(failure ~ attribute1 +
                     attribute2_cat +
                     attribute3_cat +
                     attribute4_cat +
                     attribute5 +
                     attribute6 +
                     attribute8_cat +
                     attribute9_cat,
                   data = train_dat, 
                   method = "glmnet", 
                   weights = model_weights_1,
                   tuneGrid = expand.grid(
                     .alpha = 0:1,
                     .lambda = seq(0, 0.3, length.out = 5)),
                   metric = "Kappa")

results_mod_bin_3 <- glm_results(mod_bin_3, train_dat, test_dat, "failure", "1")

results_mod_bin_3[[2]]
results_mod_bin_3[[3]]
results_mod_bin_3[[4]]
results_mod_bin_3[[5]]
results_mod_bin_3[[6]]

```

The results are very similar, but not better. In fact, we can see that for the test datasets these look similar except for some cutoff values where the log variables are better:

```{r}
results_mod_bin_2[[6]]$data %>% 
  mutate(model = "log var") %>% 
  bind_rows(
    results_mod_bin_3[[6]]$data %>% 
      mutate(model = "cat var")) %>% 
  roc_curve_many(.)

```


### GBM

We now use a less restrictive model than the linear one used previously. These kind of models are called *Generalized Boosted Models*. We use the R implementation in the package \texttt{gbm}.

```{r}
# Function to print trained model results
gbm_results <- function(model, train_data, test_data, response_variable, positive_value,
                        range = c(0:100)/100){
  
  train_data <- train_data %>% 
    mutate_("response" = response_variable)
  
  test_data <- test_data %>% 
    mutate_("response" = response_variable)
  
  cm_train <- caret::confusionMatrix(predict(model, newdata = train_data), 
                                     train_data$response,
                                     positive = positive_value)
  
  cm_test <- caret::confusionMatrix(predict(model, newdata = test_data), 
                                    test_data$response,
                                    positive = positive_value)
  
  
  roc_curve_train <- roc_curve(model, train_data, response_variable, positive_value, range)
  
  roc_curve_test <- roc_curve(model, test_data, response_variable, positive_value, range)
  
  return(list(cm_train,
              cm_test,
              roc_curve_train,
              roc_curve_test))
}

```

We first train a model using the log-transformed data and no special weights. We can see in the confusion matrices and ROC curves that it isn't efficient at all.

```{r, include=F}
ctrl <- trainControl(method = "cv",
                     number = 3,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

dat_gbm_tr <- train_dat %>% 
  mutate(failure = ifelse(failure == "1", "failure", "not_failure"))

dat_gbm_test <- test_dat %>% 
  mutate(failure = ifelse(failure == "1", "failure", "not_failure"))

mod_gbm_1 <- train(failure ~ attribute1 +
                     log_attribute2 +
                     log_attribute3 +
                     log_attribute4 +
                     attribute5 +
                     attribute6 +
                     log_attribute8 +
                     log_attribute9,
                   data = dat_gbm_tr,
                   trControl = ctrl,
                   method = "gbm", 
                   metric = "Kappa")
```

```{r}
results_mod_gbm_1 <- gbm_results(mod_gbm_1, dat_gbm_tr, dat_gbm_test, 
                                 "failure", "failure", 
                                 0:100/100)

results_mod_gbm_1[[1]]
results_mod_gbm_1[[2]]
results_mod_gbm_1[[3]]
results_mod_gbm_1[[4]]
```

Now we add the weights to the model. The predictions are much better now.

```{r, include=F}
mod_gbm_2_w <- train(failure ~ attribute1 +
                       log_attribute2 +
                       log_attribute3 +
                       log_attribute4 +
                       attribute5 +
                       attribute6 +
                       log_attribute8 +
                       log_attribute9,
                     data = dat_gbm_tr,
                     trControl = ctrl,
                     method = "gbm", 
                     weights = model_weights_1,
                     metric = "Kappa")


```

```{r}

results_mod_gbm_2_w <- gbm_results(mod_gbm_2_w, dat_gbm_tr, dat_gbm_test, 
                                   "failure", "failure", 
                                   0:200/200)

results_mod_gbm_2_w[[1]]
results_mod_gbm_2_w[[2]]
results_mod_gbm_2_w[[3]]
results_mod_gbm_2_w[[4]]
```

We may want to know what's an efficient probability cut to do the prediction. By seeing the ROC curve alone we cannot do this, because we only know the specificity and sensibility, but not the cutoff value. To see this, we'll filter the data from the ROC curve. Since we have very few failure cases, we may want to maximize the sensibility without sacrifying much specificity, so we'll look for sensibility values close to 0.8.

```{r}
results_mod_gbm_2_w[[4]]$data %>% 
  filter(sens > 0.75 & sens < 0.85) %>% 
  kable()

```

We can see that a good cutoff value is around $p > 0.3$ with which we get high sensibility without losing much specificity. We can see the confusion matrix for this particular cutoff value in the following table.

```{r}
confusionMatrix(ifelse(predict(mod_gbm_2_w, newdata = dat_gbm_test, type = 'prob')$failure > 0.3, "failure", "not_failure"),
                dat_gbm_test$failure)
```

Now, let's use a model with the categorical variables that we created earlier.

```{r, include=F}

mod_gbm_3_w <- train(failure ~ attribute1 +
                       attribute2_cat +
                       attribute3_cat +
                       attribute4_cat +
                       attribute5 +
                       attribute6 +
                       attribute8_cat +
                       attribute9_cat,
                     data = dat_gbm_tr,
                     trControl = ctrl,
                     method = "gbm", 
                     weights = model_weights_1,
                     metric = "Kappa")

```

```{r}
results_mod_gbm_3_w <- gbm_results(mod_gbm_3_w, dat_gbm_tr, dat_gbm_test, 
                                   "failure", "failure", 
                                   0:200/200)

results_mod_gbm_3_w[[1]]
results_mod_gbm_3_w[[2]]
results_mod_gbm_3_w[[3]]
results_mod_gbm_3_w[[4]]
```

Both models yield similar results. We can compare both ROC curves to see which one is better.

```{r}
log_cat_bin_df_gbm <- results_mod_gbm_2_w[[4]]$data %>% 
  mutate(model = "log var") %>% 
  bind_rows(
    results_mod_gbm_3_w[[4]]$data %>% 
      mutate(model = "cat var"))

roc_curve_many(log_cat_bin_df_gbm)
```

We can see that again both models are very similar. 

We now compare the results of the four good models we've computed so far: both linear models and both GBM models. We can see the ROC curves in the following plot. The GBM models tend to perform better, but not considerably better.

```{r}
results_mod_gbm_2_w[[4]]$data %>% 
  mutate(model = "GBM log var") %>% 
  bind_rows(
    results_mod_gbm_3_w[[4]]$data %>% 
      mutate(model = "GBM cat var")) %>% 
  bind_rows(
    results_mod_bin_2[[6]]$data %>% 
      mutate(model = "Bin log var")) %>% 
  bind_rows(
    results_mod_bin_3[[6]]$data %>% 
      mutate(model = "Bin cat var")) %>% 
  roc_curve_many(.)
```

## Conclusions

If one had to choose between these models, I'd probably go for the logistic regression because the performance wasn't much worse than the performance with GBM, but it's a much simpler model that can be easily interpreted and quickly implemented.

This was as simple exercise that was aimed to see my coding skills and knowledge. I only used two types of models, but I think the performance could be enhanced by fine tuning parameters of more complicated models, such as neural networks; but because of the tiem constraints I won't do this.





