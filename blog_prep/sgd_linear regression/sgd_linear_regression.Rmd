---
#title: 
author: "Mario Becerra"
#date: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(gridExtra)
library(Rcpp)
library(tidyverse)
```


In a [previous post](https://mariobecerra.github.io/rblogging/2017/09/11/gd_and_sgd_regression.html) I showed how to implement gradient descent and stochastic gradient descent for linear regression and logistic regression. The problem with that implementation was that I used R's for loops which can be quite slow. In this version, I implement the functions in Rcpp, reducing the running time considerably.


# Stochastic gradient descent (SGD)

In statistical learning it is common to find the need to solve optimization problems of the form

\begin{equation}
\min_{\theta \in \mathbb{R}^p} L(x, \theta), \quad \text{with} \, \, 
L(\theta) = \frac{1}{n} \sum_{i = 1}^n { \psi_i(x, \theta) }.
\end{equation}

In both past examples, the loss functions that we wanted to minimize were expressed in that way. Gradient descent uses iterations in the form

\[
\theta_{k+1} = \theta_k - \alpha_k \nabla L(\theta_k) :=\theta_k - \frac{\alpha_k}{n} \sum_{i = 1}^n \nabla \psi_i(\theta_k),
\]

which involves evaluating $n$ gradients and then taking an average. In the cases of big scale machine learning, the number of observations $n$ is really big, so computing all of those gradients in each iteration is expensive. That's why methods such as SGD come up, methods where the number of gradients to compute doesn't depend on $n$, it is constant. SGD uses iterations of the form

\[
\theta_{k+1} = \theta_k - \alpha_k \nabla \psi_{i_k}(\theta_k),
\]

where $i_k \in \left\{1, 2, ..., n \right\}$ is randomly chosen. The gradient $\nabla \psi_{i_k}(\theta_k)$ is an unbiased estimator of $\nabla L(\theta_k)$. This way, each iteration is really cheap because it involves the computation of only one gradient. It can happen that some $\nabla \psi_{i_k}(\theta_k)$ in particular doesn't give a direction of descent from $\theta_k$, but on average we do get descent directions, such that the sequence $\left\{ \theta_0, \theta_1, ... \right\}$ can be guided to a minimizer $\theta^*$.

### SGD for linear regression: 

To show this method, the same two examples are implemented using SGD.

##### **Linear regression**


As we saw before, we want to minimize the quadratic loss function. Here, each $\psi_i(x, \theta)$ is

$$
\psi_i(x, \theta) = \left( y_i - \beta_0 - \beta_1 x_{i1} - ...  \beta_p x_{ip} \right)^2 = \ell_i^2(x, \beta),
$$ 

with $\ell_i(x, \beta)$ defined earlier as

$$
\ell_i(x, \beta) = \left( y_i - \beta_0 - \beta_1 x_{i1} - ...  \beta_p x_{ip} \right).
$$
We can see that the partial derivative of $\psi_i(x, \beta)$ with respect to each $\beta_j$ with $j \in \left\{ 0, ..., p \right\}$ is

\[
\frac{\partial \psi_i}{\partial \beta_j} = \frac{\partial \ell^2_i}{\partial \beta_j} = -2 x_{ij} \ell_i(x, \beta),
\]

so, the direction of descent in each iteration is

\[
\nabla_{\beta} \psi_i(x, \beta) = 
\left( \frac{\partial \psi_i}{\partial \beta_0}, ..., \frac{\partial \psi_i}{\partial \beta_p} \right).
\]

Once again, the vector of initial parameters was $\beta^0 = (0, 0)^T$.  The stopping rule was that the norm of the gradient $\nabla_{\beta} L(x, \beta)$ was less than $0.000001$, or that there were more than 300 iterations or that the squared norm of the differences of the parameter vector from one iteration to the next (i.e. $||\beta^{k+1} - \beta^k||^2_2$) was less than $10^{-15}$. 

In SGD, an epoch is a set of $n$ accesses to the dataset. That is, in each epoch, the gradients of all elements in the dataset have been computed once.

The following code shows the functions implemented to get SGD running. I use for loops, which I know is not the best idea in R, but it's to illustrate. Maybe later I'll implement it with Rcpp and see the difference in speed.

```{r}
# Show C++ file content
cat(readLines("sgd.cpp"), sep = '\n')

# Compile with Rcpp
sourceCpp("sgd.cpp")
```


```{r}
## Function that performs minibatch gradient descent on dataset
lm_minibatch <- function(X, y, minibatch_size = 15, 
                         max_it = 3000, 
                         initial_point = NA, 
                         seed = 201802,
                         alpha = 0.001,
                         g_norm_tol = 1e-8,
                         beta_diff_norm_tol = 1e-8){
  
  # Start the algorithm
  data_matrix = as.matrix(cbind(X, y))
  n <- nrow(data_matrix)
  p <- ncol(data_matrix) # number of parameters (including intercept)
  
  if(is.na(initial_point[1])) initial_point = rep(0.0, p)
  
  if(n %% minibatch_size == 0) {
    num_iters = floor(n/minibatch_size)
  } else {
    num_iters = floor(n/minibatch_size) + 1
  }

  betas_df <- as.data.frame(matrix(rep(0, max_it*num_iters*p), ncol = p))
  names(betas_df) <- paste0("beta_", 0:(p-1))
  
  iteration_values <- tibble(
    epoch = rep(0, max_it*num_iters),
    obs = 1:(max_it*num_iters),
    gradient_norm = rep(0, max_it*num_iters)) %>% 
    bind_cols(betas_df)
  
  # observation_control <- tibble(
  #   obs = 1:(max_it*num_iters),
  #   shuffle_ix = rep(0, max_it*num_iters))
  
  i = 0
  betas <- initial_point
  while(i < max_it){
    i = i + 1
    print(i)
    # Shuffle data and keep a track of reordered observations
    set.seed(seed)
    shuffle_ix <- sample(nrow(data_matrix))
    # observation_control$shuffle_ix[(i*num_iters - num_iters + 1):(i*num_iters)] <- shuffle_ix
    shuffled_data = data_matrix[shuffle_ix,]
    epoch_betas <- epoch_update(shuffled_data, betas, alpha, i, minibatch_size)
    
    #epoch_betas_for$betas
    #epoch_betas$betas
    
    betas_old <- betas
    betas <- epoch_betas$betas
    epoch_values_temp <- as.data.frame(epoch_betas$epoch_values)
    names(epoch_values_temp) <- names(iteration_values)
    iteration_values[((i-1)*num_iters + 1):((i)*num_iters),] <- epoch_values_temp
    g_norm <- epoch_betas$epoch_values[num_iters, 3]
    # g_norm <- epoch_betas$epoch_values$gradient_norm[n]
    dif_betas_norm <- sum((betas - betas_old)^2)
    if(g_norm < g_norm_tol | dif_betas_norm < beta_diff_norm_tol) break
  }
  
  iteration_values <- iteration_values[1:(i*num_iters),]
  iteration_values$it <- 1:nrow(iteration_values)
  
  return(list(
    iteration_values = iteration_values,
    betas = betas
  ))
}


# Function to plot the values of the parameters in a simple linear regression (2 parameters)
plot_minibatches_2_params <- function(lm_minibatch
                                      # beta_0, beta_1, coef_1, coef_2
                                      ){
  data <- lm_minibatch$iteration_values
  if(length(lm_minibatch$beta) == 2) {
    # Plots all iterations in all epochs
    gg <- data %>% 
      ggplot(aes(beta_0, beta_1)) +
      xlab("Beta 0") +
      ylab("Beta 1") +
      geom_path(size = 0.1, color = 'black') +
      geom_point(size = 0.01, color = 'black', alpha = 0.2) +
      # geom_point(aes(x, y),
      #            data = tibble(x = beta_0,
      #                          y = beta_1)) +
      # geom_point(aes(x, y),
      #            data = tibble(x = coef_1,
      #                          y = coef_2),
      #            shape = 'x',
      #            size = 5,
      #            color = 'blue') +
      theme_bw()
    # theme(panel.grid.minor = element_blank())
    return(gg)
  } else{
    return("Error")
  }
}




```

```{r}
N <- 500
beta_0 <- -2
beta_1 <- 4
beta_2 <- 2

data <- tibble(x = rnorm(N),
               y = beta_0 + beta_1*x + rnorm(N, 0, 0.2))

X = data[,"x"]
y = data$y

plots_size <- lapply(0:8, function(i){
  mb_size = 2^i
  if(mb_size <= N){
    mod_minibatch <- lm_minibatch(X, y, 2^i, initial_point = c(-1, -1)) 
  } else{
    mod_minibatch <- lm_minibatch(X, y, N, initial_point = c(-1, -1)) 
  }
  gg <- plot_minibatches_2_params(mod_minibatch) +
    ggtitle(paste("Size:", mb_size))
  return(gg)
})

grid.arrange(plots_size[[1]],
             plots_size[[2]],
             plots_size[[3]],
             plots_size[[4]],
             plots_size[[5]],
             plots_size[[6]],
             plots_size[[7]],
             plots_size[[8]],
             plots_size[[9]],
             ncol = 3)

```

```{r}
data_2 <- tibble(x = rnorm(2^21),
               y = 1 + 3*x + rnorm(2^21, 0, 0.5))

minibatch_times <- tibble(log_data_size =seq(10, 20),
                              seconds = 0)

for(i in 1:nrow(minibatch_times)){
  log_size = minibatch_times$log_data_size[i]
  dat = data_2[1:2^log_size,]
  time = system.time(lm_minibatch(dat$x, dat$y, 100))
  minibatch_times$seconds[i] = time[3]
}




if("gradient_descent_times.rds" %in% list.files("cache/")){
  gradient_descent_times <- readRDS("cache/gradient_descent_times.rds")
} else {
  gradient_descent_times <- tibble(log_data_size =seq(10, 20),
                                   seconds = 0)
  for(i in 1:nrow(minibatch_times)){
    log_size = gradient_descent_times$log_data_size[i]
    dat = data_2[1:2^log_size,]
    time = system.time(lm_minibatch(dat$x, dat$y, nrow(dat)))
    gradient_descent_times$seconds[i] = time[3]
  }
  saveRDS(gradient_descent_times, "cache/gradient_descent_times.rds")
}



# mod_minibatch_2_100 <- lm_minibatch(data2$x, data2$y, 100, initial_point = c(-10, -10))
# # mod_minibatch_2_all <- lm_minibatch(data2$x, data2$y, nrow(data2), initial_point = c(-10, -10))
# # Usando como tolerancia en las betas 1e-15, llegó a las 3000 iteraciones usando el dataset completo y tardó 7 minutos (420 seg)
# plot_minibatches_2_params(mod_minibatch_2_100)



data_3 <- tibble(x1 = rnorm(N),
                x2 = rnorm(N),
                y = -2 + 4*x1 + 2*x2 + rnorm(N))


mod_minibatch_3_100 <- lm_minibatch(data_3[,1:2], data_3$y, 100, initial_point = c(-10, -10, 0))
mod_minibatch_3_100$betas


data_4 <- tibble(
  x1 = rnorm(N),
  x2 = rnorm(N),
  x3 = rnorm(N),
  y = 1 + 1*x1 + 2*x2 + 3*x3 + rnorm(N))

mod_minibatch_4_100 <- lm_minibatch(data_4[,1:3], data_4$y, 100, initial_point = c(-10, -10, 0, 0))
mod_minibatch_4_100$betas

```



We can see that in the second figure that the directions go zigzagging, but in the end reach the final solution. In GD the directions were more uniform.

In both figures there's once more the big dot that represents the real value of the parameters ($\beta_0=2$ y $\beta_1 = 1$) and the x that represents the value from the lm package, but the second image isn't that clear because of all the noise in the directions.


# Diamonds dataset

```{r}

data(diamonds)
head(diamonds)

data_diamonds_temp <- diamonds %>% 
  select(-x, -y, -z, -depth, -table, -cut, -color) %>% 
  mutate(y = log(price), x2 = log(carat), clarity = as.factor(as.character(clarity)))

clarity_dummy <- dummies::dummy(x = data_diamonds_temp$clarity) %>% as.data.frame()
names(clarity_dummy) <- paste0("x1_", 1:ncol(clarity_dummy))

data_diamonds <- clarity_dummy %>% 
  bind_cols(
    data_diamonds_temp %>% 
      select(x2, y)
  )
  
mod_minibatch_diamonds_100 <- lm_minibatch(data_diamonds[,1:9], data_diamonds$y, minibatch_size = 100)
# betas_diamonds <- as.numeric(mod_minibatch_diamonds_100[nrow(mod_minibatch_diamonds_100), grepl("beta", names(mod_minibatch_diamonds_100))])
betas_diamonds <- mod_minibatch_diamonds_100$betas

diamonds_preds <- as.matrix(cbind(rep(1, nrow(data_diamonds)), data_diamonds[,1:9])) %*% betas_diamonds

tibble(pred = as.numeric(diamonds_preds),
       real = data_diamonds$y) %>% 
  ggplot() +
  geom_point(aes(real, pred), size = 0.5, alpha = 0.5) +
  geom_abline(slope = 1) +
  theme_bw()

mod_minibatch_diamonds_100$iteration_values %>% 
  select(grep("beta", names(.)), it) %>% 
  gather(param, value, -it) %>% 
  ggplot(aes(it, value, group = param)) +
  geom_line(size = 0.5, alpha = 0.7) +
  facet_wrap(~param, scales = 'free_y') +
  theme_bw()

```

