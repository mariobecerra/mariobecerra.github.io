---
title: "GD and SGD for regression"
author: "Mario Becerra"
date: "September 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loss function optimization

In statistical learning, one usually wants to find the parameters that minimise a loss function, which almost always has to do with the error of the model we're using. In many cases, as in linear regression, using the first and second order conditions, we can find a closed formula to find the parameters. But there are many other cases in which this isn't possible, that's when numerical optimization comes in. For this post, I use definitions from *Numerical Optimization* by Nocedal & Wright (2nd Ed) and *Optimization Methods for Large-Scale Machine Learning* by Bottou, Curtis & Nocedal. 

The general problem of unrestricted optimisation is

\begin{equation}
\label{eq:prob_opt_gral}
\min_{\theta \in \mathbb{R}^p} L(\theta).
\end{equation}

For statistical learning, $L$ is usually a convex loss function such as quadratic loss, and $\theta$ is a parameter or vector of parameters. Generally, numerical optimization algorithms are iterative. A solution is a vector $\theta^*$ called local minimiser, which makes the function $L$ to be minimised in a neighbourhood around $\theta^*$. Formally, a vector $\theta^*$ is a local minimiser if there exists a neighbourhood $\mathcal{N}$ of $\theta^*$ such that $L(\theta^*) \leq L(\theta)$ for all $\theta \in \mathcal{N}$.

In numerical optimisation, we make use of the sufficient second order conditions. Suppose that the Hessian matrix $\nabla^2 L$ is continuous in an open neighbourhood of $\theta^*$, that the gradient $\nabla L(\theta^*) = 0$ and that $\nabla^2 L(\theta^*)$ is positive definite; then $\theta^*$ is a local minimiser of $L$.

This is basic calculus, but it provides the base of numerical optimisation algorithms. In general, all algorithms search for a point $\theta^*$ such that $\nabla L(\theta^*) = 0$. 



# Gradient descent (GD)

Gradient descent is an algorithm that belongs to a family called *line search algorithms*. In each iteration, thee algorithms search for a direction in which to go and then update the current value in accordance to that direction. That is, in the $k$-th iteration, we have a value $\theta_k$, and we look for a direction $p_k$ to update to a new value $\theta_{k+1} = \theta_k + \alpha_k p_k$, where $\alpha_k > 0$ is the 'distance' that the algorithm moves toward direction $p_k$, and is called *step length*. Once that the value of the parameter is updated, we find a new direction in which to move forward and then update the parameter value again. This is done until a stopping criteria is met, this usually being that the gradient vector norm is smaller than a certain small positive scalar.

In gradient descent, the direction $p_k$ in which the algorithm moves is the maximum descent direction, that is, the negative of the gradient $-\nabla L(\theta_k)$. So, in each iteration we have

\begin{equation}
\label{eq:iteracion_descenso_grad}
\theta_{k+1} = \theta_k - \alpha_k \nabla L(\theta_k).
\end{equation}

We find a problem when we want to compute the step length $\alpha_k$: we want to find a value that the function $L$ decreases as much as possible, but we don't want to waste much time choosing the value. The best option is the globa minimiser of the auxiliary function $\phi(\alpha_k) = L(\theta_k + \alpha_k p_k)$, but it's too expensive to compute. Generally, heuristics are used to choose the sequence of values for $\alpha_k$ and try which one satisfies those conditions.

One of those conditions is called Armijo conditions, and finds the $\alpha_k$ that allows a sufficient descent in the function $L$, measured as

$$
L(\theta_k + \alpha_k p_k) \leq L(\theta_k) + c_1 \alpha_k \nabla L(\theta_k)^T p_k,
$$

for a constant $c_1 \in (0, 1)$. Usually $c_1$ is small, such as $10^{-4}$. This condition may not be enough, because for very small values of $\alpha_k$ the condition may be met, and we don't usually want very small step lengths. Some ways to fix this is to backtrack. This consists of choosing a big value of $\alpha_k$ (such as $\alpha_k = 1$), and then an iterative subalgorithm is initiated which decreases the value of $\alpha_k$ until de Armijo condition is met.

There are some other ways to choose step length $\alpha_k$, but those are beyond the scope of this post. Nocedal & Wright's book has a lot more information about it.

### Examples: 

Now that we've covered some theory, to explain further the concept of gradient descent, we work with two examples: linear regression and logistic regression.

##### **Linear regression**

In linear regression we want to minimise the quadratic loss function

\begin{equation}
L(x, \beta) = \frac{1}{n} \sum_{i = 1}^n \left( y_i - \beta_0 - \beta_1 x_{i1} - ...  \beta_p x_{ip} \right)^2.
\end{equation}

Taking its partial derivatives, we have for each $j \in \left\{1, ..., p \right\}$

\begin{equation}
\frac{\partial L}{\partial \beta_j} = 
-\frac{2}{n} \sum_{i = 1}^n \left( x_{ij} \ell_i(x, \beta) \right)
\end{equation}

where $x_{i1} = 1$ for each $i \in \left\{1, ..., n \right\}$ and

\begin{equation}
\ell_i(x, \beta) = \left( y_i - \beta_0 - \beta_1 x_{i1} - ...  \beta_p x_{ip} \right).
\end{equation}

From here, we have that the gradient descent direction for each iteration is

$$
\nabla_{\beta} L(x, \beta) = \left( \frac{\partial L}{\partial \beta_0}, ..., \frac{\partial L}{\partial \beta_p} \right)^T.
$$

In the implemented example, I generated a vector $x \in \mathbb{R}^n$ with $n = 1000$ such that $x_i \sim N(0, 1)$ for each $i \in \left\{1, ..., n \right\}$ and the response variable was $y = \beta_0 + \beta_1x + \varepsilon$ with $\varepsilon \sim N(0, 1)$, $\beta_0 = 2$ and $\beta_1 = 1$. We can see the code used and the generated data in the following image.

```{r, message = FALSE}
library(tidyverse)

theme_set(theme_bw())


N <- 1000
beta_0 <- 2
beta_1 <- 1

set.seed(20170909)
data <- tibble(x = rnorm(N),
               y = beta_0 + beta_1*x + rnorm(N))

model <- lm(y~x, data = data)


(data %>% 
    ggplot(aes(x, y)) +
    geom_point(size = 0.7, alpha = 0.6) +
    geom_abline(slope = beta_1, intercept = beta_0)
) 
```

So, to minimise the quadratic loss function, we start with a vector $\beta^0 \in \mathbb{R}^2$, and in each iteration we update as

$$
\beta^{k+1} = \beta^k - \alpha_k \nabla_{\beta} L(x, \beta)
$$

until we meet a stopping criteria. In this case, the stopping criteria was that the norm of the gradient $\nabla_{\beta} L(x, \beta)$ was smaller than $0.000001$ or that it exceeded 100 iterations. The vector of initial parameters was $\beta^0 = (0, 0)^T$, and a fixed $\alpha_k$ of $0.1$ for all $k$ without taking into account the Armijo contition.

The gradient was implemented in R as a function as follows. It receives a dataframe and a vector of parameters, and returns a vector of directions.

```{r}
gradient <- function(data, betas){
  n <- nrow(data)
  const <- rep(1, nrow(data))
  li <- data$y - betas[1]*const - betas[2]*data$x
  g1 <- -2*sum(li)/n
  g2 <- -2*sum(li*data$x)/n
  return(c(g1, g2))
}
```

We call the function and store the values of the parameters and descent directions in a dataframe.

```{r}
max_it <- 100
data_gradient_descent <- tibble(it = 1:max_it,
                                beta_0 = rep(0, max_it),
                                beta_1 = rep(0, max_it),
                                gradient_norm = rep(0, max_it))

i = 0
betas <- c(0, 0)
alpha = 0.1
while(i < max_it){
  i = i + 1
  g <- gradient(data, betas)
  g_norm <- sqrt(sum(g^2))
  g_unit <- g/g_norm
  data_gradient_descent$beta_0[i] <- betas[1]
  data_gradient_descent$beta_1[i] <- betas[2]
  data_gradient_descent$gradient_norm[i] <- g_norm
  if(g_norm < 0.000001) break
  betas <- betas - alpha*g
}


data_gradient_descent <- data_gradient_descent[1:i,]
```


We can see the values of the parameter vector in each iteration in the following image. The big dot is the real value of the parameters (2 and 1) and the x is the value that I get using the lm package in R. We can see that the implemented algorithm converges to these values. In fact, the value we get is exactly the same as the one lm gets.

```{r, warning=FALSE}
data_gradient_descent %>% 
  ggplot(aes(beta_0, beta_1)) +
  xlab("Beta 0") +
  ylab("Beta 1") +
  geom_segment(
    aes(
      xend = c(tail(beta_0, n = -1), NA),
      yend = c(tail(beta_1, n = -1), NA)
    ),
    size = 0.4,
    color = '#919191',
    arrow = arrow(length = unit(0.18, "cm"))
  ) +
  geom_point(size = 0.4, color = 'black') +
  geom_point(aes(x, y),
             data = tibble(x = beta_0,
                           y = beta_1)) +
  geom_point(aes(x, y),
             data = tibble(x = model$coefficients[1],
                           y = model$coefficients[2]),
             shape = 'x',
             size = 5) +
  theme(
    panel.grid.minor = element_blank()
  )
```

And here we can see the norm of the gradient vector in each iteration. We can see that it keeps decreasing.

```{r}
data_gradient_descent %>% 
  ggplot(aes(it, gradient_norm)) +
  geom_point(size = 0.7) +
  geom_line(size = 0.4)

# Delete all generated objects
rm(list = ls()) 
```


##### **Logistic regression**

In the case of logistic regression, we want to minimise a loss function called deviance that is defined as

\begin{equation}
L(x, \beta) = - \frac{2}{n} \sum_{i=1}^{n} \left[ y_i \log(h(\beta^T x_i)) + (1-y_i) \log(1-h(\beta^T x_i)) \right] = 
- \frac{2}{n} \sum_{i=1}^{n}{\ell_i(\beta)}
\end{equation}

where

$$
\beta^T x_i = \sum_{j=0}^{p}{\beta_j x_{ij}},
$$ 

$$
h(w) = \frac{e^w}{1+e^w}
$$ 

and 

$$
\ell_i(x) = y_i \log(h(\beta^T x_i)) + (1-y_i) \log(1-h(\beta^T x_i)).
$$

We get the partial derivatives

$$
\frac{\partial L}{\partial \beta_j} = -\frac{2}{n} \sum_{i = 1}^n { \frac{\partial \ell_i}{\partial \beta_j} }
$$

and, using the fact that $h'(w) = h(w)(1-h(w))$, we get

\begin{equation}
\begin{split}
\frac{\partial \ell_i}{\partial \beta_j} & = 
\frac{y_i h'(\beta^T x_i) x_{ij} }  {h(\beta^T x_i)} + \frac{(1 - y_i) (-1) h'(\beta^T x_i) x_{ij}} {1 - h(\beta^T x_i)} \\
& = \frac{h'(\beta^T x_i) x_{ij} y_i}{h(\beta^T x_i)} - \frac{(1 - y_i) h'(\beta^T x_i) x_{ij}}{1 - h((\beta^T x_i))} \\
& = h'(\beta^T x_i) x_{ij} \left(\frac{y_i}{h(\beta^T x_i)} - \frac{1-y_i}{1-h(\beta^T x_i)} \right) \\
& = h'(\beta^T x_i) x_{ij} \left(\frac{y_i - y_i h(\beta^T x_i) - 
h(\beta^T x_i) + y_i h(\beta^T x_i)}{h(\beta^T x_i)(1-h(\beta^T x_i))} \right) \\
& = x_{ij}(y_i - h(\beta^T x_i)).
\end{split}
\end{equation}

So, we have that

$$
\frac{\partial L}{\partial \beta_j} = -\frac{2}{n} \sum_{i = 1}^n { x_{ij}(y_i - h(\sum_{j=0}^{p}{\beta_j x_{ij}})) },
$$

where once more $x_{i1} = 1$ for all $i \in \left\{1, ..., n \right\}$.


We once again generate a vector $x \in \mathbb{R}^n$ with $n = 1000$ such that $x_i \sim N(0, 1)$ for each $i \in \left\{1, ..., n \right\}$ and then an auxiliary vector was computed, $p_i = \frac{1}{\exp \left( - \beta_0 - \beta_1 x_i \right)}$, with $\beta_0 = 1$ and $\beta_1 = 4$. Finally, the response variable $y$ was built simulating Bernoulli random variables, such that $y_i \sim Bern(p_i)$. The generated data can be seen in the following image.

```{r}
N <- 1000
beta_0 <- 1
beta_1 <- 4

set.seed(124362)
data <- tibble(x = rnorm(N),
               z = beta_0 + beta_1*x,
               pr = 1/(1 + exp(-z)),
               y = rbinom(1000, 1, pr))

model <- glm(y ~ x, data = data, family = "binomial")


data %>% 
  ggplot(aes(x, y)) +
  geom_point(size = 0.5, alpha = 0.4) +
  stat_smooth(method="glm", 
              method.args = list(family = "binomial"), 
              se = FALSE,
              size = 0.6,
              color = 'black')
```


To minimise the deviance, we do exactly as in the previous case: we start with an initial vector of parameters $\beta^0 \in \mathbb{R}^2$, and in each iteration we update

$$
\beta^{k+1} = \beta^k - \alpha_k \nabla_{\beta} L(x, \beta)
$$

until certain criteria is met. In this case, the stopping criteria was harder: the norm of the gradient $\nabla_{\beta} L(x, \beta)$ should be less than $0.0001$, or the ratio of norms between one iteration and the next had to be bigger than $0.8$, or to exceed 500 iterations. The vector of initial parameters was $\beta^0 = (-10, 10)^T$. We did backtracking with an initial $\alpha_0 = 3$.

The backtracking function implemented is shown below.

```{r}
backtrack <- function(betas, g, alpha, deviance, data, rho = 0.5, max_iter = 50, c1 = 10^-4){
  funct_value <- do.call(deviance, list(betas, data))
  norm_pk <- sum(g^2)
  i = 0
  while(i < max_iter){
    i = i + 1
    left <- do.call(deviance, list(betas - alpha*g, data))
    right <- funct_value + c1*alpha*norm_pk
    if(left <= right) break
    alpha = alpha*rho
  }
  return(list(alpha = alpha,
              iter = i))
}
```

The auxiliary $h$ function, the gradient and the deviance are shown here.

```{r}
h <- function(x){
  return(1/(1 + exp(-x)))
}

gradient <- function(data, betas){
  n <- nrow(data)
  const <- rep(1, nrow(data))
  bx <- betas[1]*const + betas[2]*data$x
  li <- data$y - h(bx)
  g1 <- -2*sum(li)/n
  g2 <- -2*sum(li*data$x)/n
  return(c(g1, g2))
}


deviance <- function(betas, data = data){
  n <- nrow(data)
  const <- rep(1, nrow(data))
  bx <- betas[1]*const + betas[2]*data$x
  hbx <- h(bx)
  aux <- (1 - data$y)*log(1 - hbx)
  aux2 <- ifelse(is.nan(aux), 0, aux)
  li <- data$y*log(hbx) + aux2
  d <- -2*sum(li)/n
  return(d)
}
```

We call the functions to start the algorithm and save the parameters of interest in a dataframe.

```{r}
max_it <- 1000
data_gradient_descent <- tibble(it = 1:max_it,
                                beta_0 = rep(0, max_it),
                                beta_1 = rep(0, max_it),
                                gradient_norm = rep(0, max_it),
                                deviance = rep(0, max_it),
                                alpha = rep(0, max_it))

i = 0
betas <- c(-10, 10) # reales 1 y 4
g <- gradient(data, betas)
g_norm <- sqrt(sum(g^2))
while(i < max_it){
  i = i + 1
  g_norm_0 <- g_norm # old gradient norm
  g <- gradient(data, betas)
  g_norm <- sqrt(sum(g^2))
  g_unit <- g/g_norm
  data_gradient_descent$beta_0[i] <- betas[1]
  data_gradient_descent$beta_1[i] <- betas[2]
  data_gradient_descent$gradient_norm[i] <- g_norm
  data_gradient_descent$deviance[i] <- deviance(betas, data)
  if(g_norm < 0.0001 & g_norm/g_norm_0 > 0.8) break
  backtrack_result <- backtrack(betas, g, 3, deviance, data)
  alpha <- backtrack_result[[1]]
  data_gradient_descent$alpha[i] <- alpha
  betas <- betas - alpha*g
}


data_gradient_descent <- data_gradient_descent[1:i,]
```


We can see the values of the parameters in each iteration in the following image.

```{r, warning=FALSE}
data_gradient_descent %>% 
  ggplot(aes(beta_0, beta_1)) +
  xlab("Beta 0") +
  ylab("Beta 1") +
  geom_segment(
    aes(
      xend = c(tail(beta_0, n = -1), NA),
      yend = c(tail(beta_1, n = -1), NA)
    ),
    size = 0.4,
    color = '#919191',
    arrow = arrow(length = unit(0.18, "cm"))
  ) +
  geom_point(size = 0.4, color = 'black') +
  geom_point(aes(x, y),
             data = tibble(x = beta_0,
                           y = beta_1)) +
  geom_point(aes(x, y),
             data = tibble(x = model$coefficients[1],
                           y = model$coefficients[2]),
             shape = 'x',
             size = 5) +
  theme(panel.grid.minor = element_blank())
```

The big point is the real value of the parameters (1 and 4), and the x is the value that we get by using the glm package in R. The algorithm converges to that value, and once again the value that we get is exactly the same as in the glm package.

The following images show the decreasing values of the gradient norm and the deviance with each iteration.

```{r, warning=FALSE}

data_gradient_descent %>% 
  ggplot(aes(it, gradient_norm)) +
  geom_point(size = 0.7) +
  geom_line(size = 0.4)


data_gradient_descent %>% 
  ggplot(aes(it, deviance)) +
  geom_point(size = 0.7) +
  geom_line(size = 0.4)
```

# Stochastic gradient descent (SGD)

In statistical learning it is common to find the need to solve optimisation problems of the form

\begin{equation}
  \min_{\theta \in \mathbb{R}^p} L(x, \theta), \quad \text{with} \, \, 
  L(\theta) = \frac{1}{n} \sum_{i = 1}^n { \psi_i(x, \theta) }.
\end{equation}

En los ejemplos de la subsección \ref{sec:descenso_gradiente} se puede ver esto con claridad: las funciones de pérdida a minimizar son de la forma \ref{eq:perd_gral_apr_est}. El método de descenso en gradiente presentado en \ref{sec:descenso_gradiente} utiliza iteraciones de la forma de la ecuación \ref{eq:iteracion_descenso_grad}, es decir, 

\[
\theta_{k+1} = \theta_k - \alpha_k \nabla L(\theta_k) :=\theta_k - \frac{\alpha_k}{n} \sum_{i = 1}^n \nabla \psi_i(\theta_k),
\]

lo cual involucra la evaluación de $n$ gradientes para después promediarlos. En los casos de aprendizaje automático de gran escala, el número de observaciones $n$ es grande, por lo que calcular estos gradientes puede resultar costoso. Debido a esto, surgen los métodos como descenso en gradiente estocástico, en los cuales el número de gradientes a evaluar no depende de $n$, sino que es constante. Este método utiliza iteraciones de la forma

\[
\theta_{k+1} = \theta_k - \frac{\alpha_k}{n} \nabla \psi_{i_k}(\theta_k),
\]

donde $i_k \in \left\{1, 2, ..., n \right\}$ es escogido aleatoriamente. El gradiente $\nabla \psi_{i_k}(\theta_k)$ es un estimador insesgado de $\nabla L(\theta_k)$. De esta forma, cada iteración es muy barata pues involucra la evaluación de solamente un gradiente. Puede suceder que alguna $\nabla \psi_{i_k}(\theta_k)$ particular no otorgue una dirección de descenso partiendo de $\theta_k$, pero, intuitivamente, en promedio sí se dan direcciones de descenso, de tal forma que le sucesión $\left\{ \theta_0, \theta_1, ... \right\}$ puede ser guiada a un minimizador $\theta^*$.

Para ilustrar este método, se implementaron los dos mismos ejemplos que en la sección anterior, es decir, la regresión lineal y la regresión logística.


### Examples: 


##### **Linear regression**


En el caso de la regresión lineal, se desea minimizar la pérdida cuadrática definida en la ecuación \ref{eq:perd_cuadratica_reg_lineal}. Aquí, si se utiliza la notación utilizada en la ecuación \ref{eq:perd_gral_apr_est}, cada $\psi_i(x, \theta)$ es 

$$
\psi_i(x, \theta) = \left( y_i - \beta_0 - \beta_1 x_{i1} - ...  \beta_p x_{ip} \right)^2 = \ell_i^2(x, \beta),
$$ 

con $\ell_i(x, \beta)$ definido en la ecuación \ref{eq:ele_i_reg_lineal}. Se puede ver que la derivada parcial de $\psi_i(x, \beta)$ respecto a cada $\beta_j$ con $j \in \left\{ 0, ..., p \right\}$ es

\[
\frac{\partial \psi_i}{\partial \beta_j} = \frac{\partial \ell^2_i}{\partial \beta_j} = -2 x_{ij} \ell_i(x, \beta),
\]

por lo que la dirección de avance en cada iteración del algoritmo es

\[
\nabla_{\beta} \psi_i(x, \beta) = 
\left( \frac{\partial \psi_i}{\partial \beta_0}, ..., \frac{\partial \psi_i}{\partial \beta_p} \right).
\]

Nuevamente, el vector de parámetros iniciales fue $\beta^0 = (0, 0)^T$.  El criterio de paro era que la norma del gradiente $\nabla_{\beta} L(x, \beta)$ fuera menor a $0.000001$, que se excediera de 300 iteraciones o que la norma dos al cuadrado de las diferencias del vector de parámetros entre una iteración y otra (i.e. $\norm{\beta^{k+1} - \beta^k}^2_2$) fuera menor a $10^{-15}$. 

Los valores del vector de parámetros en cada iteración se pueden ver en las figuras FIGURAS!!!!!!


Para entender mejor estas imágenes, es pertinente explicar qué es un epoch. Un epoch es un conjunto de $n$ accesos al conjunto de datos. Es decir, en cada epoch, se evalúan los gradientes de todos los datos en el conjunto de entrenamiento.

En la figura ITERACIONES!!! se muestran los valores de los parámetros para todos los gradientes en todos los epochs. En la figura LALALALALALA se muestran solo los valores al final de cada epoch para tener mayor claridad en la visualización. Se puede apreciar en la figura LALALALALA que, a diferencia de la figura LALALALALA donde las direcciones de descenso se ven más uniformes hacia una misma dirección, las direcciones van hacia todos lados en una especie de zig-zag que eventualmente llega a la solución.

En ambas figuras se muestra un punto grande que representa el valor real de los parámetros ($\beta_0=2$ y $\beta_1 = 1$) y el tache que representa el valor obtenido utilizando el paquete lm de R, pero en la figura LALALALALALA no se alcanza a distinguir tan bien, pero en ambas imágenes se puede ver que el algoritmo implementado converge a los valores deseados.



%%%%%%%%%%%%%%%%
%%% Reg logística
%%%%%%%%%%%%%%%%

Para la regresión logística ya se tienen hechos casi todos los cálculos necesarios en la subsección \ref{sec:descenso_gradiente}. Con la notación utilizada en la ecuación \ref{eq:perd_gral_apr_est}, cada $\psi_i(x, \theta)$ es 

$$
\psi_i(x, \theta) = y_i \log(h(\beta^T x_i)) + (1-y_i) \log(1-h(\beta^T x_i)) = \ell_i(x, \beta).
$$ 

Además, en las ecuaciones \ref{eq:derivacion_ell_i_reg_log} se llega a que

\[
\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial \psi_i}{\partial \beta_j} = x_{ij}(y_i - h(\beta^T x_i)).
\]

Por lo que ya se tiene todo lo necesario. En la implementación, el criterio de paro fue el mismo que en descenso en gradiente estocástico para regresión lineal: que la norma del gradiente $\nabla_{\beta} L(x, \beta)$ fuera menor a $0.000001$, que se excediera de 300 iteraciones o que la norma dos al cuadrado de las diferencias del vector de parámetros entre una iteración y otra fuera menor a $10^{-15}$. 

Los valores del vector de parámetros en cada iteración se pueden ver en las figuras LALALALALAL y LALALALALAL. Las explicaciones de estas dos imágenes son análogas a las de regresión lineal. Nuevamente, el valor obtenido con el método implementado converge al valor real y al que se obtuvo con el paquete glm.


```{r, eval = F}

library(tidyverse)

theme_set(theme_bw(base_size = 20))

system("mkdir ../../out/optim")

###################################
###################################
### Linear Regression
###################################
###################################

N <- 1000
beta_0 <- 2
beta_1 <- 1

set.seed(124362)
data <- tibble(x = rnorm(N),
               y = beta_0 + beta_1*x + rnorm(N))

model <- lm(y~x, data = data)



gradient_row <- function(data_row, betas){
  li <- data_row$y - betas[1] - betas[2]*data_row$x
  g1 <- -2*li
  g2 <- -2*li*data_row$x
  return(c(g1, g2))
}

epoch_update <- function(data, betas, alpha, n_epoch, verbose = 1, reordering = F){
  n <- nrow(data)
  if(reordering) data$ix <- sample(1:n)
  else data$ix <- 1:n
  epoch_values <- tibble(
    n_epoch = rep(n_epoch, n),
    obs = 1:n,
    beta_0 = rep(0, n),
    beta_1 = rep(0, n),
    gradient_norm = rep(0, n))
  
  # Iterate over rows in data
  for(i in 1:n){
    # Update coefficientes
    g <- gradient_row(data[data$ix == i,], betas) 
    betas <- betas - alpha*g
    # Print and write values in table to keep track and make plots
    g_norm <- sqrt(sum(g^2))
    g_unit <- g/g_norm
    epoch_values$beta_0[i] <- betas[1]
    epoch_values$beta_1[i] <- betas[2]
    epoch_values$gradient_norm[i] <- g_norm
    if(verbose == 2){
      cat(
        "\n\tEpoch: ", n_epoch,
        "\n\tObs: ", i, 
        "\n\tbeta_0: ", betas[1], 
        "\n\tbeta_1: ", betas[2],
        "\n\tgradient_norm: ", g_norm, 
        "\n\tDirection: ", g_unit[1], g_unit[2],
        "\n")
    }
  } # End for
  
  
  if(verbose == 1){
    cat(
      "\n\tEpoch: ", n_epoch,
      "\n\tbeta_0: ", epoch_values$beta_0[n],
      "\n\tbeta_1: ", epoch_values$beta_1[n],
      "\n\tgradient_norm: ", epoch_values$gradient_norm[n],
      "\n")
  }
  return(list(
    epoch_values = epoch_values,
    betas = betas
  ))
}


max_it <- 300
n <- nrow(data)
data_gradient_descent <- tibble(
  epoch = rep(0, max_it*n),
  obs = 1:(max_it*n),
  beta_0 = rep(0, max_it*n),
  beta_1 = rep(0, max_it*n),
  gradient_norm = rep(0, max_it*n))

i = 0
betas <- c(0, 0)
alpha = 0.001
while(i < max_it){
  i = i + 1
  epoch_betas <- epoch_update(data, betas, alpha, i, verbose = 1)
  betas_old <- betas
  betas <- epoch_betas$betas
  data_gradient_descent[((i-1)*n + 1):((i)*n),] <- epoch_betas$epoch_values
  g_norm <- epoch_betas$epoch_values$gradient_norm[n]
  dif_betas_norm <- sum((betas - betas_old)^2)
  if(g_norm < 0.000001 | dif_betas_norm < 1e-15) break
}


data_gradient_descent <- data_gradient_descent[1:(i*n),]
data_gradient_descent$it <- 1:nrow(data_gradient_descent)

# data_gradient_descent %>% 
#   filter(it %% n == 1) %>% 
#   ggplot(aes(it, gradient_norm)) +
#   geom_point(size = 0.7) +
#   geom_line(size = 0.4)


plot_gd_iter <- function(data_gradient_descent, model, beta_0, beta_1, denom = 0){
  # N <- nrow(data_gradient_descent)
  
  if(denom > 0) {
    data <- data_gradient_descent %>% 
      filter(it %% floor(n/denom) == 1)
  } else {
    data <- data_gradient_descent
  }
  
  gg <- data %>% 
    ggplot(aes(beta_0, beta_1)) +
    xlab("Beta 0") +
    ylab("Beta 1") +
    geom_segment(
      aes(
        xend = c(tail(beta_0, n = -1), NA),
        yend = c(tail(beta_1, n = -1), NA)
      ),
      size = 0.4,
      color = '#919191',
      arrow = arrow(length = unit(0.18, "cm"))
    ) +
    #geom_path(size = 0.6, color = 'red') +
    geom_point(size = 0.3, color = 'black') +
    geom_point(aes(x, y),
               data = tibble(x = beta_0,
                             y = beta_1)) +
    geom_point(aes(x, y),
               data = tibble(x = model$coefficients[1],
                             y = model$coefficients[2]),
               shape = 'x',
               size = 5) +
    theme(
      #panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
  return(gg)
}



# Plots only at the beginning of each epoch
plot_gd_iter(data_gradient_descent, model, beta_0, beta_1, 1) %>% 
  ggsave(.,
         file = "../../out/optim/lin_reg_example_1_SGD_each_epoch.pdf",
         device = 'pdf',
         width = 8,
         height = 5)

# Plots all iterations in all epochs
(data_gradient_descent %>% 
    ggplot(aes(beta_0, beta_1)) +
    xlab("Beta 0") +
    ylab("Beta 1") +
    geom_path(size = 0.1, color = 'black') +
    geom_point(size = 0.01, color = 'black', alpha = 0.2) +
    geom_point(aes(x, y),
               data = tibble(x = beta_0,
                             y = beta_1)) +
    geom_point(aes(x, y),
               data = tibble(x = model$coefficients[1],
                             y = model$coefficients[2]),
               shape = 'x',
               size = 5) +
    theme(
      panel.grid.minor = element_blank()
    )
) %>% 
  ggsave(.,
         file = "../../out/optim/lin_reg_example_1_SGD_iter_all.pdf",
         device = 'pdf',
         width = 8,
         height = 5)

###################################
###################################
### Logistic Regression
###################################
###################################

rm(list = setdiff(ls(), list("epoch_update", "plot_gd_iter")))

N <- 1000
beta_0 <- 1
beta_1 <- 4

set.seed(124362)
data <- tibble(x = rnorm(N),
               z = beta_0 + beta_1*x,
               pr = 1/(1 + exp(-z)),
               y = rbinom(1000, 1, pr))

model <- glm(y ~ x, data = data, family = "binomial")


h <- function(x){
  return(1/(1 + exp(-x)))
}

# gradient <- function(data, betas){
#   n <- nrow(data)
#   const <- rep(1, nrow(data))
#   bx <- betas[1]*const + betas[2]*data$x
#   li <- data$y - h(bx)
#   g1 <- -2*sum(li)/n
#   g2 <- -2*sum(li*data$x)/n
#   return(c(g1, g2))
# }


gradient_row <- function(data_row, betas){
  bx <- betas[1] + betas[2]*data_row$x
  li <- li <- data_row$y - h(bx)
  g1 <- -2*li
  g2 <- -2*li*data_row$x
  return(c(g1, g2))
}


max_it <- 300
n <- nrow(data)
data_gradient_descent <- tibble(
  epoch = rep(0, max_it*n),
  obs = 1:(max_it*n),
  beta_0 = rep(0, max_it*n),
  beta_1 = rep(0, max_it*n),
  gradient_norm = rep(0, max_it*n))

i = 0
betas <- c(-10, 10)
alpha = 0.01
while(i < max_it){
  i = i + 1
  epoch_betas <- epoch_update(data, betas, alpha, i, verbose = 1)
  betas_old <- betas
  betas <- epoch_betas$betas
  data_gradient_descent[((i-1)*n + 1):((i)*n),] <- epoch_betas$epoch_values
  g_norm <- epoch_betas$epoch_values$gradient_norm[n]
  dif_betas_norm <- sum((betas - betas_old)^2)
  if(g_norm < 0.000001 | dif_betas_norm < 1e-15) break
}

data_gradient_descent <- data_gradient_descent[1:(i*n),]
data_gradient_descent$it <- 1:nrow(data_gradient_descent)

# Plots only at the beginning of each epoch
plot_gd_iter(data_gradient_descent, model, beta_0, beta_1, 1) %>% 
  ggsave(.,
         file = "../../out/optim/log_reg_example_1_SGD_each_epoch.pdf",
         device = 'pdf',
         width = 8,
         height = 5)

# Plots all iterations in all epochs
(data_gradient_descent %>% 
    ggplot(aes(beta_0, beta_1)) +
    xlab("Beta 0") +
    ylab("Beta 1") +
    geom_point(size = 0.01, color = 'black', alpha = 0.2) +
    geom_point(aes(x, y),
               data = tibble(x = beta_0,
                             y = beta_1)) +
    geom_point(aes(x, y),
               data = tibble(x = model$coefficients[1],
                             y = model$coefficients[2]),
               shape = 'x',
               size = 5) +
    theme(
      panel.grid.minor = element_blank()
    )
) %>% 
  ggsave(.,
         file = "../../out/optim/log_reg_example_1_SGD_iter_all.pdf",
         device = 'pdf',
         width = 8,
         height = 5)

```

